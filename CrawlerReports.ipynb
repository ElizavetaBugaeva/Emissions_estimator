{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in /Users/elizavetabugaeva/anaconda3/envs/spiced_final/lib/python3.10/site-packages (4.12.0)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in /Users/elizavetabugaeva/anaconda3/envs/spiced_final/lib/python3.10/site-packages (from selenium) (1.26.15)\n",
      "Requirement already satisfied: trio~=0.17 in /Users/elizavetabugaeva/anaconda3/envs/spiced_final/lib/python3.10/site-packages (from selenium) (0.22.2)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in /Users/elizavetabugaeva/anaconda3/envs/spiced_final/lib/python3.10/site-packages (from selenium) (0.10.4)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in /Users/elizavetabugaeva/anaconda3/envs/spiced_final/lib/python3.10/site-packages (from selenium) (2023.5.7)\n",
      "Requirement already satisfied: attrs>=20.1.0 in /Users/elizavetabugaeva/anaconda3/envs/spiced_final/lib/python3.10/site-packages (from trio~=0.17->selenium) (23.1.0)\n",
      "Requirement already satisfied: sortedcontainers in /Users/elizavetabugaeva/anaconda3/envs/spiced_final/lib/python3.10/site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in /Users/elizavetabugaeva/anaconda3/envs/spiced_final/lib/python3.10/site-packages (from trio~=0.17->selenium) (3.4)\n",
      "Requirement already satisfied: outcome in /Users/elizavetabugaeva/anaconda3/envs/spiced_final/lib/python3.10/site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: sniffio in /Users/elizavetabugaeva/anaconda3/envs/spiced_final/lib/python3.10/site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in /Users/elizavetabugaeva/anaconda3/envs/spiced_final/lib/python3.10/site-packages (from trio~=0.17->selenium) (1.1.3)\n",
      "Requirement already satisfied: wsproto>=0.14 in /Users/elizavetabugaeva/anaconda3/envs/spiced_final/lib/python3.10/site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /Users/elizavetabugaeva/anaconda3/envs/spiced_final/lib/python3.10/site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in /Users/elizavetabugaeva/anaconda3/envs/spiced_final/lib/python3.10/site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pdfminer.six\n",
      "  Downloading pdfminer.six-20221105-py3-none-any.whl (5.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /Users/elizavetabugaeva/anaconda3/envs/spiced_final/lib/python3.10/site-packages (from pdfminer.six) (3.1.0)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in /Users/elizavetabugaeva/anaconda3/envs/spiced_final/lib/python3.10/site-packages (from pdfminer.six) (41.0.1)\n",
      "Requirement already satisfied: cffi>=1.12 in /Users/elizavetabugaeva/anaconda3/envs/spiced_final/lib/python3.10/site-packages (from cryptography>=36.0.0->pdfminer.six) (1.15.1)\n",
      "Requirement already satisfied: pycparser in /Users/elizavetabugaeva/anaconda3/envs/spiced_final/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.21)\n",
      "Installing collected packages: pdfminer.six\n",
      "Successfully installed pdfminer.six-20221105\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pdfminer.six\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "WebDriver.get() missing 1 required positional argument: 'url'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39m# Navigate to the website\u001b[39;00m\n\u001b[1;32m     19\u001b[0m url \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mhttps://www.sustainability-reports.com\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m---> 20\u001b[0m driver\u001b[39m.\u001b[39;49mget(url)\n\u001b[1;32m     22\u001b[0m \u001b[39m# Initialize a pandas DataFrame to store the extracted data\u001b[39;00m\n\u001b[1;32m     23\u001b[0m data \u001b[39m=\u001b[39m {\n\u001b[1;32m     24\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mCompany Name\u001b[39m\u001b[39m'\u001b[39m: [],\n\u001b[1;32m     25\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mYear\u001b[39m\u001b[39m'\u001b[39m: [],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mScope 3\u001b[39m\u001b[39m'\u001b[39m: []\n\u001b[1;32m     29\u001b[0m }\n",
      "\u001b[0;31mTypeError\u001b[0m: WebDriver.get() missing 1 required positional argument: 'url'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from pdfminer.high_level import extract_text\n",
    "import re\n",
    "\n",
    "# Set the path to the ChromeDriver executable\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.binary_location = '/Applications'  # Path to the Chrome binary if needed\n",
    "chrome_driver_path = '/usr/local/bin'  # Path to the ChromeDriver executable\n",
    "\n",
    "# Initialize the WebDriver with the ChromeOptions\n",
    "driver = webdriver.Chrome\n",
    "\n",
    "# Navigate to the website\n",
    "url = 'https://www.sustainability-reports.com'\n",
    "driver.get(url)\n",
    "\n",
    "# Initialize a pandas DataFrame to store the extracted data\n",
    "data = {\n",
    "    'Company Name': [],\n",
    "    'Year': [],\n",
    "    'Scope 1': [],\n",
    "    'Scope 2': [],\n",
    "    'Scope 3': []\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Define regular expressions to search for relevant information\n",
    "scope_1_pattern = r\"Scope\\s*1:\\s*(.*?)\\s\"\n",
    "scope_2_pattern = r\"Scope\\s*2:\\s*(.*?)\\s\"\n",
    "scope_3_pattern = r\"Scope\\s*3:\\s*(.*?)\\s\"\n",
    "\n",
    "# Find and click on each report link\n",
    "report_links = driver.find_elements(By.XPATH, '//a[contains(@href, \"reports\")]')\n",
    "for link in report_links:\n",
    "    link.click()\n",
    "\n",
    "    # Wait for the report page to load (you may need to adjust the timeout)\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, '//a[contains(text(), \"Download PDF\")]')))\n",
    "\n",
    "    # Get the company name and year from the page\n",
    "    company_name = driver.find_element(By.XPATH, '//h1').text\n",
    "    year = driver.find_element(By.XPATH, '//strong[contains(text(), \"Year:\")]/following-sibling::span').text\n",
    "\n",
    "    # Find the PDF download link and click it\n",
    "    pdf_link = driver.find_element(By.XPATH, '//a[contains(text(), \"Download PDF\")]')\n",
    "    pdf_link.click()\n",
    "    \n",
    "    # Wait for the PDF to download (you may need to adjust the wait time)\n",
    "    time.sleep(5)  # You can improve this by monitoring the file download status\n",
    "    \n",
    "    # Define the path to the downloaded PDF file (you may need to adjust this)\n",
    "    pdf_path = '/path/to/downloaded.pdf'\n",
    "    \n",
    "    # Extract text from the PDF using pdfminer.six\n",
    "    with open(pdf_path, 'rb') as pdf_file:\n",
    "        pdf_text = extract_text(pdf_file)\n",
    "\n",
    "    # Extract Scope 1, Scope 2, and Scope 3 information\n",
    "    scope_1_match = re.search(scope_1_pattern, pdf_text, re.IGNORECASE)\n",
    "    scope_2_match = re.search(scope_2_pattern, pdf_text, re.IGNORECASE)\n",
    "    scope_3_match = re.search(scope_3_pattern, pdf_text, re.IGNORECASE)\n",
    "\n",
    "    # Store the extracted data in the DataFrame\n",
    "    df = df.append({\n",
    "        'Company Name': company_name,\n",
    "        'Year': year,\n",
    "        'Scope 1': scope_1_match.group(1) if scope_1_match else None,\n",
    "        'Scope 2': scope_2_match.group(1) if scope_2_match else None,\n",
    "        'Scope 3': scope_3_match.group(1) if scope_3_match else None\n",
    "    }, ignore_index=True)\n",
    "\n",
    "    # Go back to the previous page\n",
    "    driver.back()\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TimeoutException",
     "evalue": "Message: \nStacktrace:\n0   chromedriver                        0x0000000101142d98 chromedriver + 4337048\n1   chromedriver                        0x000000010113ae14 chromedriver + 4304404\n2   chromedriver                        0x0000000100d67a5c chromedriver + 293468\n3   chromedriver                        0x0000000100dacd50 chromedriver + 576848\n4   chromedriver                        0x0000000100de7908 chromedriver + 817416\n5   chromedriver                        0x0000000100da0a5c chromedriver + 526940\n6   chromedriver                        0x0000000100da1908 chromedriver + 530696\n7   chromedriver                        0x0000000101108de4 chromedriver + 4099556\n8   chromedriver                        0x000000010110d2a0 chromedriver + 4117152\n9   chromedriver                        0x000000010111352c chromedriver + 4142380\n10  chromedriver                        0x000000010110dda0 chromedriver + 4119968\n11  chromedriver                        0x00000001010e5a74 chromedriver + 3955316\n12  chromedriver                        0x000000010112aa48 chromedriver + 4237896\n13  chromedriver                        0x000000010112abc4 chromedriver + 4238276\n14  chromedriver                        0x000000010113aa8c chromedriver + 4303500\n15  libsystem_pthread.dylib             0x00000001980e426c _pthread_start + 148\n16  libsystem_pthread.dylib             0x00000001980df08c thread_start + 8\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTimeoutException\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 41\u001b[0m\n\u001b[1;32m     38\u001b[0m link\u001b[39m.\u001b[39mclick()\n\u001b[1;32m     40\u001b[0m \u001b[39m# Wait for the report page to load (you may need to adjust the timeout)\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m WebDriverWait(driver, \u001b[39m10\u001b[39;49m)\u001b[39m.\u001b[39;49muntil(EC\u001b[39m.\u001b[39;49mpresence_of_element_located((By\u001b[39m.\u001b[39;49mXPATH, \u001b[39m'\u001b[39;49m\u001b[39m//a[contains(text(), \u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mDownload PDF\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m)]\u001b[39;49m\u001b[39m'\u001b[39;49m)))\n\u001b[1;32m     43\u001b[0m \u001b[39m# Get the company name and year from the page\u001b[39;00m\n\u001b[1;32m     44\u001b[0m company_name \u001b[39m=\u001b[39m driver\u001b[39m.\u001b[39mfind_element(By\u001b[39m.\u001b[39mXPATH, \u001b[39m'\u001b[39m\u001b[39m//h1\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mtext\n",
      "File \u001b[0;32m~/anaconda3/envs/spiced_final/lib/python3.10/site-packages/selenium/webdriver/support/wait.py:95\u001b[0m, in \u001b[0;36mWebDriverWait.until\u001b[0;34m(self, method, message)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[39mif\u001b[39;00m time\u001b[39m.\u001b[39mmonotonic() \u001b[39m>\u001b[39m end_time:\n\u001b[1;32m     94\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m---> 95\u001b[0m \u001b[39mraise\u001b[39;00m TimeoutException(message, screen, stacktrace)\n",
      "\u001b[0;31mTimeoutException\u001b[0m: Message: \nStacktrace:\n0   chromedriver                        0x0000000101142d98 chromedriver + 4337048\n1   chromedriver                        0x000000010113ae14 chromedriver + 4304404\n2   chromedriver                        0x0000000100d67a5c chromedriver + 293468\n3   chromedriver                        0x0000000100dacd50 chromedriver + 576848\n4   chromedriver                        0x0000000100de7908 chromedriver + 817416\n5   chromedriver                        0x0000000100da0a5c chromedriver + 526940\n6   chromedriver                        0x0000000100da1908 chromedriver + 530696\n7   chromedriver                        0x0000000101108de4 chromedriver + 4099556\n8   chromedriver                        0x000000010110d2a0 chromedriver + 4117152\n9   chromedriver                        0x000000010111352c chromedriver + 4142380\n10  chromedriver                        0x000000010110dda0 chromedriver + 4119968\n11  chromedriver                        0x00000001010e5a74 chromedriver + 3955316\n12  chromedriver                        0x000000010112aa48 chromedriver + 4237896\n13  chromedriver                        0x000000010112abc4 chromedriver + 4238276\n14  chromedriver                        0x000000010113aa8c chromedriver + 4303500\n15  libsystem_pthread.dylib             0x00000001980e426c _pthread_start + 148\n16  libsystem_pthread.dylib             0x00000001980df08c thread_start + 8\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from pdfminer.high_level import extract_text\n",
    "import re\n",
    "\n",
    "# Set the path to the ChromeDriver executable\n",
    "chrome_driver_path = '/usr/local/bin/chromedriver'  # Path to the ChromeDriver executable\n",
    "\n",
    "# Initialize the WebDriver with the ChromeDriver executable\n",
    "driver = webdriver.Chrome() #(executable_path=chrome_driver_path)\n",
    "\n",
    "# Navigate to the website\n",
    "url = 'https://www.responsibilityreports.com/Companies?sect=3'\n",
    "driver.get(url=url)\n",
    "\n",
    "# Initialize a pandas DataFrame to store the extracted data\n",
    "data = {\n",
    "    'Company Name': [],\n",
    "    'Year': [],\n",
    "    'Scope 1': [],\n",
    "    'Scope 2': [],\n",
    "    'Scope 3': []\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Define regular expressions to search for relevant information\n",
    "scope_1_pattern = r\"Scope\\s*1:\\s*(.*?)\\s\"\n",
    "scope_2_pattern = r\"Scope\\s*2:\\s*(.*?)\\s\"\n",
    "scope_3_pattern = r\"Scope\\s*3:\\s*(.*?)\\s\"\n",
    "\n",
    "# Find and click on each report link\n",
    "report_links = driver.find_elements(By.XPATH, '//a[contains(@href, \"reports\")]')\n",
    "for link in report_links:\n",
    "    link.click()\n",
    "\n",
    "    # Wait for the report page to load (you may need to adjust the timeout)\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, '//a[contains(text(), \"Download PDF\")]')))\n",
    "\n",
    "    # Get the company name and year from the page\n",
    "    company_name = driver.find_element(By.XPATH, '//h1').text\n",
    "    year = driver.find_element(By.XPATH, '//strong[contains(text(), \"Year:\")]/following-sibling::span').text\n",
    "\n",
    "    # Find the PDF download link and click it\n",
    "    pdf_link = driver.find_element(By.XPATH, '//a[contains(text(), \"Download PDF\")]')\n",
    "    pdf_link.click()\n",
    "    \n",
    "    # Wait for the PDF to download (you may need to adjust the wait time)\n",
    "    time.sleep(5)  # You can improve this by monitoring the file download status\n",
    "    \n",
    "    # Define the path to the downloaded PDF file (you may need to adjust this)\n",
    "    pdf_path = '/data.pdf'\n",
    "    \n",
    "    # Extract text from the PDF using pdfminer.six\n",
    "    with open(pdf_path, 'rb') as pdf_file:\n",
    "        pdf_text = extract_text(pdf_file)\n",
    "\n",
    "    # Extract Scope 1, Scope 2, and Scope 3 information\n",
    "    scope_1_match = re.search(scope_1_pattern, pdf_text, re.IGNORECASE)\n",
    "    scope_2_match = re.search(scope_2_pattern, pdf_text, re.IGNORECASE)\n",
    "    scope_3_match = re.search(scope_3_pattern, pdf_text, re.IGNORECASE)\n",
    "\n",
    "    # Store the extracted data in the DataFrame\n",
    "    df = df.append({\n",
    "        'Company Name': company_name,\n",
    "        'Year': year,\n",
    "        'Scope 1': scope_1_match.group(1) if scope_1_match else None,\n",
    "        'Scope 2': scope_2_match.group(1) if scope_2_match else None,\n",
    "        'Scope 3': scope_3_match.group(1) if scope_3_match else None\n",
    "    }, ignore_index=True)\n",
    "\n",
    "    # Go back to the previous page\n",
    "    driver.back()\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "next steps\n",
    "1. Write the scraping code to download all reports from :https://www.responsibilityreports.com/Companies?sect=3\n",
    "2. Write scraping code to extract emissions data from those reports\n",
    "3. Add the new file to the data collection copy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spiced_final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
